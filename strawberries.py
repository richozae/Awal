# -*- coding: utf-8 -*-
"""strawberries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AbGpVvkYnShxJM8-0-jldB_rsDMoTUTX

The selection of quality strawberries.
"""

#This data analyst for the selection of quality strawberries
#there are 9 point parameter data
# fixed acidity,	volatile acidity,	citric acid,	residual sugar,
# chlorides,	free sulfur dioxide,	total sulfur dioxide,	density,	pH,
# sulphates,	alcohol, and	quality

#import library python
#numpy for make a array, matplotlib for visualization data ex: table or plot
#seaborn for complicated matplotlib, and pandas for make table data
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from google.colab import drive

drive.mount('/content/drive')
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/progresproject/posting/quality_strawberries.csv')

#print df value for the next information
print(df)

#To display the first five rows of a DataFrame
df.head()

#To display the last five rows of a DataFrame
df.tail()

#to display a raw and column
df.shape

#provides detailed information about the DataFrame
df.info()

#for calculate the number of missing values
df.isna().sum()

#identify rows that have duplicates
duplicates=df.duplicated()
df[duplicates]

print(duplicates)

df=df.drop_duplicates()

#used to access the list of column names
df.columns

#generate summary statistics from numeric columns
df.describe()

#gets a list of unique values contained in the "quality" column of the DataFrame
df['quality'].unique()

"""Data Visualization"""

#get color for bar plot visualization data
grey_palette = sns.color_palette(['#AAAAB6','#B79EBC','#9C7DA1','#A46D87','#DEB8AD','#EDD8BB'])
sns.set_palette(grey_palette)

plt.figure(figsize=(10,5))
sns.barplot(x=df['quality'].value_counts().index,y=df['quality'].value_counts().values)

#parameter fixed acidity
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['fixed acidity'])
plt.subplot(2,2,2)
sns.boxplot(df['fixed acidity'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['fixed acidity'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['fixed acidity'])
print('Correlation between fixed acidity and quality is',df['quality'].corr(df['fixed acidity']))
print('Skewness of the fixed column is',df['fixed acidity'].skew())

#conclution fixed acidity parameter
#1. There is no correlation or relationship between "quality" and "fixed acidity"
#2. All available grades of strawberry have an acidity level that remains within a fixed range of 7-9

#parameters volatile acidity
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['volatile acidity'])
plt.subplot(2,2,2)
sns.boxplot(df['volatile acidity'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['volatile acidity'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['volatile acidity'])
print('Correlation between volatile acidity and quality is',df['quality'].corr(df['volatile acidity']))
print('Skewness of the volatile acidity column is',df['volatile acidity'].skew())

#1. there are corelation "volatile acility" dan "quality", pada quality 3 range total is 0.65 until 1
#quality 4 is 0.50 until 0.85, quality 5 is 0.45 until 0.65, quality 6 in range 0.45 until 0.60
#quality 7 in range 0.25 until 0,50, and quality 8 in range 0.30 until 0.50
#2. there is negative corelation between "volatile acility" dan "quality"
#3. The lower the volatile acidity value, the better the quality of the strawberries selected

#parameters citric acid
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['citric acid'])
plt.subplot(2,2,2)
sns.boxplot(df['citric acid'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['citric acid'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['citric acid'])
print('Correlation between citric acid and quality is',df['quality'].corr(df['citric acid']))
print('Skewness of the citric acid column is',df['citric acid'].skew())

#1. there is positive corelation between "citric acid" dan "quality"
#2. The higher the citric acid value, the higher the quality of the strawberries selected

#parameter residual sugar
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['residual sugar'])
plt.subplot(2,2,2)
sns.boxplot(df['residual sugar'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['residual sugar'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['residual sugar'])
print('Correlation between residual sugar and quality is',df['quality'].corr(df['residual sugar']))
print('Skewness of the residual sugar column is',df['residual sugar'].skew())

#parameter chlorides
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['chlorides'])
plt.subplot(2,2,2)
sns.boxplot(df['chlorides'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['chlorides'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['chlorides'])
print('Correlation between chlorides and quality is',df['quality'].corr(df['chlorides']))
print('Skewness of the chlorides column is',df['chlorides'].skew())

#parameter free sulfur dioxide
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['free sulfur dioxide'])
plt.subplot(2,2,2)
sns.boxplot(df['free sulfur dioxide'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['free sulfur dioxide'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['free sulfur dioxide'])
print('Correlation between free sulfur dioxide and quality is',df['quality'].corr(df['free sulfur dioxide']))
print('Skewness of the free sulfur dioxide column is',df['free sulfur dioxide'].skew())

#parameter total sulfur dioxide
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['total sulfur dioxide'])
plt.subplot(2,2,2)
sns.boxplot(df['total sulfur dioxide'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['total sulfur dioxide'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['total sulfur dioxide'])
print('Correlation between total sulfur dioxide and quality is',df['quality'].corr(df['total sulfur dioxide']))
print('Skewness of the total sulfur dioxide column is',df['total sulfur dioxide'].skew())

#parameter density
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['density'])
plt.subplot(2,2,2)
sns.boxplot(df['density'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['density'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['density'])
print('Correlation between density and quality is',df['quality'].corr(df['density']))
print('Skewness of the density column is',df['density'].skew())

#parameter PH
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['pH'])
plt.subplot(2,2,2)
sns.boxplot(df['pH'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['pH'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['pH'])
print('Correlation between pH and quality is',df['quality'].corr(df['pH']))
print('Skewness of the pH column is',df['pH'].skew())

#parameter sulphates
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['sulphates'])
plt.subplot(2,2,2)
sns.boxplot(df['sulphates'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['sulphates'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['sulphates'])
print('Correlation between sulphates and quality is',df['quality'].corr(df['sulphates']))
print('Skewness of the sulphates column is',df['sulphates'].skew())

#parameter alcohol
plt.figure(figsize=(10,5))
plt.subplot(2,2,1)
sns.histplot(x=df['alcohol'])
plt.subplot(2,2,2)
sns.boxplot(df['alcohol'])
plt.subplot(2,2,3)
sns.boxplot(x=df['quality'],y=df['alcohol'])
plt.subplot(2,2,4)
sns.violinplot(x=df['quality'],y=df['alcohol'])
print('Correlation between alcohol and quality is',df['quality'].corr(df['alcohol']))
print('Skewness of the alcohol column is',df['alcohol'].skew())

#ANALYST CONFUSION MATRIKS
plt.figure(figsize=(8,5))
sns.heatmap(df.corr())

# from seaborn library for describe to connection x,y and know relationship between these variables
plt.figure(figsize=(8,5))
sns.scatterplot(x=df['citric acid'],y=df['fixed acidity'],hue=df['quality'],size=(df['quality']*1000))

# from seaborn library for describe to connection x,y and know relationship between these variables
plt.figure(figsize=(8,5))
sns.scatterplot(x=df['density'],y=df['fixed acidity'],hue=df['quality'],size=df['quality']*1000)

#take a dimensional from dataframe
df.shape

"""REMOVING OUTLIER"""

# address outliers that could impact statistical analysis.
from scipy import stats
z = np.abs(stats.zscore(df[df.dtypes[df.dtypes != 'object'].index]))
df = df[(z < 3).all(axis=1)]

# Values greater than or equal to 7 are considered good quality
# and are converted to 1, while values less than 7 are converted to 0.
X=df.drop('quality',axis=1)
y=df['quality']
y=df['quality'].apply(lambda y_value:1 if y_value>=7 else 0)

plt.figure(figsize=(10,5))
sns.countplot(x=y)

"""HANDLING IMBALANCE CLASESS"""

#addressing class imbalance in classification problems
from imblearn.over_sampling import SMOTE
smote=SMOTE()
x_smote,y_smote=smote.fit_resample(X,y)

plt.figure(figsize=(10,5))
sns.countplot(x=y_smote)

"""TRAIN TEST SPLIT DATA"""

#for split data -> with testing is 30% for dataset
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(x_smote,y_smote,test_size=0.3,random_state=42)

"""# LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
lr_model=LogisticRegression(solver='newton-cg')

lr_model.fit(X_train,y_train)

lr_model.score(X_test,y_test)

y_lr_pred=lr_model.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_lr_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,y_lr_pred))

"""# DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
dtr=DecisionTreeClassifier()

parameters = {
    'criterion': ['gini', 'entropy', 'log_loss'],
    'splitter': ['best', 'random'],
    'max_depth': [2, 4, 6, 8],
    'min_samples_split': [2, 4, 6, 8],
    'min_samples_leaf': [1, 2, 3, 4],
    'max_features': ['sqrt'],  # Mengatur max_features menjadi 'sqrt'
}

from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(dtr, parameters, cv=5, scoring='f1_macro')
grid_search.fit(X_train, y_train)

print('The Parameters are:', grid_search.best_params_)

dtr=DecisionTreeClassifier(criterion = grid_search.best_params_.get('criterion'),
                                  splitter = grid_search.best_params_.get('splitter'),
                                  max_depth = grid_search.best_params_.get('max_depth'),
                                  max_features = grid_search.best_params_.get('max_features'),
                                  min_samples_leaf = grid_search.best_params_.get('min_samples_leaf'),
                                  min_samples_split = grid_search.best_params_.get('min_samples_split'),
                                  random_state = 42)

dtr.fit(X_train,y_train)

y_pred=dtr.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,y_pred))

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

rfc=RandomForestClassifier()

rfc.fit(X_train,y_train)

y_rfc_pred=rfc.predict(X_test)

print(classification_report(y_test,y_rfc_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,y_rfc_pred))

"""# ADABOOST CLASSIFIER"""

from sklearn.ensemble import AdaBoostClassifier
abc=AdaBoostClassifier()

abc.fit(X_train,y_train)

y_abc_pred=abc.predict(X_test)

print(classification_report(y_test,y_abc_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test,y_abc_pred))

"""# MODEL ANALYST AND EVALUATION"""

plt.figure(figsize=(15,10))
plt.subplot(2,2,1)
sns.heatmap(confusion_matrix(y_test,y_lr_pred),annot=True)
plt.ylabel('Predicted Values')
plt.xlabel('Actual Values')
plt.title('Confusion Matrix for Logistic Regression')
plt.subplot(2,2,2)
sns.heatmap(confusion_matrix(y_test,y_pred),annot=True)
plt.ylabel('Predicted Values')
plt.xlabel('Actual Values')
plt.title('Confusion Matrix for Decision Tree')
plt.subplot(2,2,3)
sns.heatmap(confusion_matrix(y_test,y_rfc_pred),annot=True)
plt.ylabel('Predicted Values')
plt.xlabel('Actual Values')
plt.title('Confusion Matrix for Random Forest')
plt.subplot(2,2,4)
sns.heatmap(confusion_matrix(y_test,y_abc_pred),annot=True)
plt.ylabel('Predicted Values')
plt.xlabel('Actual Values')
plt.title('Confusion Matrix for AdaBoost')

from sklearn.metrics import accuracy_score
models = ['Logistic Regression', ' Decision Tree', 'Random Forest', 'AdaBoost']
accuracy = [accuracy_score(y_test, y_lr_pred), accuracy_score(y_test, y_pred), accuracy_score(y_test, y_rfc_pred), accuracy_score(y_test, y_abc_pred)]
plt.figure(figsize=(8,3))
sns.barplot(x=models, y=accuracy)
plt.title('Model Accuracy Comparison Method')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0.7, 1.0)

#Random Forest is getting us the maximum accuracy i.e. 90 %.

plt.figure(figsize=(7, 3))
sns.lineplot(x=models, y=accuracy, marker="o", linestyle="-")
plt.title('Model Accuracy Comparison Method (Line Plot)')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0.7, 1.0)
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

#Random Forest is getting us the maximum accuracy i.e. 90 %.

plt.figure(figsize=(4,4))
plt.pie(accuracy, labels=models, autopct='%1.1f%%', startangle=140)
plt.title('Model Accuracy Comparison Method')
plt.show()

#Random Forest provides the most optimal classification percentage level, namely 26.1%.